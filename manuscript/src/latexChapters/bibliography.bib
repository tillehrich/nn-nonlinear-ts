% This file was created with JabRef 2.6.
% Encoding: Cp1252

@article{Ahmed2010,
author = {Ahmed, Nesreen and Atiya, Amir and Gayar, Neamat and El-Shishiny, Hisham},
year = {2010},
month = {08},
pages = {594-621},
title = {An Empirical Comparison of Machine Learning Models for Time Series Forecasting},
volume = {29},
journal = {Econometric Reviews},
doi = {10.1080/07474938.2010.481556}
}

@article{Makridakis2018,
    author = {Makridakis, Spyros and Spiliotis, Evangelos and Assimakopoulos, Vassilios},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {Statistical and Machine Learning forecasting methods: Concerns and ways forward},
    year = {2018},
    month = {03},
    volume = {13},
    url = {https://doi.org/10.1371/journal.pone.0194889},
    pages = {1-26},
    abstract = {Machine Learning (ML) methods have been proposed in the academic literature as alternatives to statistical ones for time series forecasting. Yet, scant evidence is available about their relative performance in terms of accuracy and computational requirements. The purpose of this paper is to evaluate such performance across multiple forecasting horizons using a large subset of 1045 monthly time series used in the M3 Competition. After comparing the post-sample accuracy of popular ML methods with that of eight traditional statistical ones, we found that the former are dominated across both accuracy measures used and for all forecasting horizons examined. Moreover, we observed that their computational requirements are considerably greater than those of statistical methods. The paper discusses the results, explains why the accuracy of ML models is below that of statistical ones and proposes some possible ways forward. The empirical results found in our research stress the need for objective and unbiased ways to test the performance of forecasting methods that can be achieved through sizable and open competitions allowing meaningful comparisons and definite conclusions.},
    number = {3},
    doi = {10.1371/journal.pone.0194889}
}

@article{Hyndman2020,
  title={Modern strategies for time series regression},
  author={Clark, Stephanie and Hyndman, Rob J and Pagendam, Dan and Ryan, Louise M},
  journal={International Statistical Review},
  year={2020},
  publisher={Wiley Online Library}
}

@book{Hastie2009,
  title={The elements of statistical learning: data mining, inference, and prediction},
  author={Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  year={2009},
  publisher={Springer Science \& Business Media}
}

@article{Hill1996,
author = {Hill, Tim and O'Connor, Marcus and Remus, William},
year = {1996},
month = {11},
pages = {1082-1092},
title = {Neural Network Models for Time Series Forecasts},
volume = {42},
journal = {Management Science},
doi = {10.1287/mnsc.42.7.1082}
}

@article{Bucci2019,
    author = {Bucci, Andrea},
    title = "{Realized Volatility Forecasting with Neural Networks}",
    journal = {MPRA Paper No. 95443},
    year = {2019},
    month = {08},
    abstract = "{In the last few decades, a broad strand of literature in finance has implemented artificial neural networks as forecasting method. The major advantage of this approach
    is the possibility to approximate any linear and nonlinear behaviors without knowing
    the structure of the data generating process. This makes it suitable for forecasting
    time series which exhibit long memory and nonlinear dependencies, like conditional
    volatility. In this paper, I compare the predictive performance of feed-forward and recurrent neural networks (RNN), particularly focusing on the recently developed Long
    short-term memory (LSTM) network and NARX network, with traditional econometric
    approaches. The results show that recurrent neural networks are able to outperform
    all the traditional econometric methods. Additionally, capturing long-range dependence
    through Long short-term memory and NARX models seems to improve the forecasting
    accuracy also in a highly volatile framework.}",
    url = {https://mpra.ub.uni-muenchen.de/95443/},
}

@article{Sestanovic2021,
author = {Šestanović, Tea and Arnerić, Josip},
title = {Neural network structure identification in inflation forecasting},
journal = {Journal of Forecasting},
volume = {40},
number = {1},
pages = {62-79},
keywords = {euro zone, feedforward neural network, inflation forecasting, Jordan neural network},
doi = {https://doi.org/10.1002/for.2698},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/for.2698},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/for.2698},
abstract = {Abstract Neural networks (NNs) are appropriate to use in time series analysis under conditions of unfulfilled assumptions, i.e., non-normality and nonlinearity. The aim of this paper is to propose means of addressing identified shortcomings with the objective of identifying the NN structure for inflation forecasting. The research is based on a theoretical model that includes the characteristics of demand-pull and cost-push inflation; i.e., it uses the labor market, financial and external factors, and lagged inflation variables. It is conducted at the aggregate level of euro area countries from January 1999 to January 2017. Based on the estimated 90 feedforward NNs (FNNs) and 450 Jordan NNs (JNNs), which differ in variable parameters (number of iterations, learning rate, initial weight value intervals, number of hidden neurons, and weight value of the context unit), the mean square error (MSE), and the Akaike Information Criterion (AIC) are calculated for two periods: in-the-sample and out-of-sample. Ranking NNs simultaneously on both periods according to either MSE or AIC does not lead to the selection of the ‘best’ NN because the optimal NN in-the-sample, based on MSE and/or AIC criteria, often has high out-of-sample values of both indicators. To achieve the best compromise solution, i.e., to select an optimal NN, the preference ranking organization method for enrichment of evaluations (PROMETHEE) is used. Comparing the optimal FNN and JNN, i.e., FNN(4,5,1) and JNN(4,3,1), it is concluded that under approximately equal conditions, fewer hidden layer neurons are required in JNN than in FNN, confirming that JNN is parsimonious compared to FNN. Moreover, JNN has a better forecasting performance than FNN.},
year = {2021}
}

@book{Luetkepohl2004,
author={Lütkepohl, Helmut and Krätzig, Markus},
place={Cambridge},
series={Themes in Modern Econometrics},
title={Applied Time Series Econometrics},
DOI={10.1017/CBO9780511606885},
publisher={Cambridge University Press},
year={2004},
collection={Themes in Modern Econometrics}
}

@article{Demetrescu2013,
author = {Demetrescu, Matei and Kruse, Robinson},
title = {The power of unit root tests against nonlinear local alternatives},
journal = {Journal of Time Series Analysis},
volume = {34},
number = {1},
pages = {40-61},
keywords = {Nonlinear models, stochastic trend, near integration, persistent nonlinearity, local power, C12 (Hypothesis Testing), C22 (Time-Series Models)},
doi = {https://doi.org/10.1111/j.1467-9892.2012.00812.x},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9892.2012.00812.x},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9892.2012.00812.x},
abstract = {This article extends the analysis of local power of unit root tests in a nonlinear direction by considering local nonlinear alternatives and tests built specifically against stationary nonlinear models. In particular, we focus on the popular test proposed by Kapetanios et al. (2003, Journal of Econometrics 112, 359–379) in comparison to the linear Dickey–Fuller test. To this end, we consider different adjustment schemes for deterministic terms. We provide asymptotic results which imply that the error variance has a severe impact on the behaviour of the tests in the nonlinear case; the reason for such behaviour is the interplay of non-stationarity and nonlinearity. In particular, we show that nonlinearity of the data generating process can be asymptotically negligible when the error variance is moderate or large (compared to the ‘amount of nonlinearity’), rendering the linear test more powerful than the nonlinear one. Should however the error variance be small, the nonlinear test has better power against local alternatives. We illustrate this in an asymptotic framework of what we call persistent nonlinearity. The theoretical findings of this article explain previous results in the literature obtained by simulation. Furthermore, our own simulation results suggest that the user-specified adjustment scheme for deterministic components (e.g. OLS, GLS, or recursive adjustment) has a much higher impact on the power of unit root tests than accounting for nonlinearity, at least under local (linear or nonlinear) alternatives.},
year = {2013}
}

@article{Corradi2000,
title = {Testing for stationarity-ergodicity and for comovements between nonlinear discrete time Markov processes},
journal = {Journal of Econometrics},
volume = {96},
number = {1},
pages = {39-73},
year = {2000},
issn = {0304-4076},
doi = {https://doi.org/10.1016/S0304-4076(99)00050-0},
url = {https://www.sciencedirect.com/science/article/pii/S0304407699000500},
author = {Valentina Corradi and Norman R. Swanson and Halbert White},
keywords = {Cointegration, Linear stochastic comovement, Markov processes, Nonlinearities},
abstract = {In this paper we introduce a class of nonlinear data generating processes (DGPs) that are first order Markov and can be represented as the sum of a linear plus a bounded nonlinear component. We use the concepts of geometric ergodicity and of linear stochastic comovement, which correspond to the linear concepts of integratedness and cointegratedness, to characterize the DGPs. We show that the stationarity test due to Kwiatowski et al. (1992, Journal of Econometrics, 54, 159–178) and the cointegration test of Shin (1994, Econometric Theory, 10, 91–115) are applicable in the current context, although the Shin test has a different limiting distribution. We also propose a consistent test which has a null of linear cointegration (comovement), and an alternative of `non-linear cointegration'. Monte Carlo evidence is presented which suggests that the test has useful finite sample power against a variety of nonlinear alternatives. An empirical illustration is also provided.}
}

@article{Hyndman2008,
   author = {Rob J. Hyndman and Yeasmin Khandakar},
   title = {Automatic Time Series Forecasting: The forecast Package for R},
   journal = {Journal of Statistical Software, Articles},
   volume = {27},
   number = {3},
   year = {2008},
   keywords = {},
   abstract = {Automatic forecasts of large numbers of univariate time series are often needed in business and other contexts. We describe two automatic forecasting algorithms that have been implemented in the forecast package for R. The first is based on innovations state space models that underly exponential smoothing methods. The second is a step-wise algorithm for forecasting with ARIMA models. The algorithms are applicable to both seasonal and non-seasonal data, and are compared and illustrated using four real time series. We also briefly describe some of the other functionality available in the forecast package.},
   issn = {1548-7660},
   pages = {1--22},
   doi = {10.18637/jss.v027.i03},
   url = {https://www.jstatsoft.org/v027/i03}
}

@article{Lee1993,
title = {Testing for neglected nonlinearity in time series models: A comparison of neural network methods and alternative tests},
journal = {Journal of Econometrics},
volume = {56},
number = {3},
pages = {269-290},
year = {1993},
issn = {0304-4076},
doi = {https://doi.org/10.1016/0304-4076(93)90122-L},
url = {https://www.sciencedirect.com/science/article/pii/030440769390122L},
author = {Tae-Hwy Lee and Halbert White and Clive W.J. Granger},
abstract = {In this paper a new test, the neural network test for neglected nonlinearity, is compared with the Keenan test, the Tsay test, the White dynamic information matrix test, the McLeod-Li test, the Ramsey RESET test, the Brock-Dechert-Scheinkman test, and the Bispectrum test. The neural network test is based on the approximating ability of neural network modeling techniques recently developed by cognitive scientists. This test is a Lagrange multiplier test that statistically determines whether adding ‘hidden units’ to the linear network would be advantageous. The performance of the tests is compared using a variety of nonlinear artificial series including bilinear, threshold autoregressive, and nonlinear moving average models, and the tests are applied to actual economic time series. The relative performance of the neural network test is encouraging. Our results suggest that it can play a valuable role in evaluating model adequacy. The neural network test has proper size and good power, and many of the economic series tested exhibit potential nonlinearities.}
}

@article{Engle1982,
  title={Autoregressive conditional heteroscedasticity with estimates of the variance of United Kingdom inflation},
  author={Engle, Robert F},
  journal={Econometrica: Journal of the Econometric Society},
  pages={987--1007},
  year={1982},
  publisher={JSTOR}
}

@article{Blake2003,
author = {Blake, Andrew P. and Kapetanios, George},
year = {2003},
month = {7},
title = {Testing for Arch in the Presence of Nonlinearity of Unknown Form in the Conditional Mean},
keywords = {Working Paper No. 496},
doi = {https://dx.doi.org/10.2139/ssrn.425402}
}


@article{Bollerslev1986,
title = "Generalized autoregressive conditional heteroskedasticity",
journal = "Journal of Econometrics",
volume = "31",
number = "3",
pages = "307 - 327",
year = "1986",
issn = "0304-4076",
doi = "https://doi.org/10.1016/0304-4076(86)90063-1",
url = "http://www.sciencedirect.com/science/article/pii/0304407686900631",
author = "Tim Bollerslev",
abstract = "A natural generalization of the ARCH (Autoregressive Conditional Heteroskedastic) process introduced in Engle (1982) to allow for past conditional variances in the current conditional variance equation is proposed. Stationarity conditions and autocorrelation structure for this new class of parametric models are derived. Maximum likelihood estimation and testing are also considered. Finally an empirical example relating to the uncertainty of the inflation rate is presented."
}

@article{Garcia2020,
author = {Sánchez García, Javier and Cruz Rambaud, Salvador},
title = {A GARCH approach to model short-term interest rates: Evidence from Spanish economy},
journal = {International Journal of Finance \& Economics},
year = {2020},
keywords = {econometrics, financial and monetary economics, GARCH models, macroeconomics, real interest rates, time series analysis},
doi = {https://doi.org/10.1002/ijfe.2234},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/ijfe.2234},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/ijfe.2234},
abstract = {Abstract This paper focuses on GARCH modelling of the nominal short-term interest rates of the Spanish government three-year bonds. This methodology allows an ex-ante approximation to this variable which proves to be a valuable alternative against econometric specifications that imply a homoscedastic error term. Then, real short-term interest rates are estimated by employing the reduced Fisher equation. Eventually, the results obtained are compared with the observed values of the real time-series in order to measure their accuracy.}
}

@article{Bisaglia2014,
author = {Bisaglia, Luisa and Gerolimetto, Margherita},
year = {2014},
month = {01},
pages = {5-32},
title = {Testing for (non)linearity in economic time series},
volume = {16},
journal = {Quaderni Di Statistica}
}

@article{Brock1996,
author = {   W. A.   Brock  and    J. A.   Scheinkman  and    W. D.   Dechert  and    B.   LeBaron },
title = {A test for independence based on the correlation dimension},
journal = {Econometric Reviews},
volume = {15},
number = {3},
pages = {197-235},
year  = {1996},
publisher = {Taylor & Francis},
doi = {10.1080/07474939608800353},

URL = {
        https://doi.org/10.1080/07474939608800353

},
eprint = {
        https://doi.org/10.1080/07474939608800353

}

}

@TechReport{Caporale2004,
  author={Caporale, Guglielmo Maria and Ntantamis, Christos and Pantelidis, Theologos and Pittis, Nikitas},
  title={{The BDS Test as a Test for the Adequacy of a GARCH(1,1) Specification. A Monte Carlo Study}},
  year=2004,
  month=May,
  institution={Institute for Advanced Studies},
  type={Economics Series},
  url={https://ideas.repec.org/p/ihs/ihsesp/156.html},
  number={156},
  abstract={In this study, we examine the Brock, Dechert and Scheinkman (BDS) test when applied to the standardised residuals of an estimated GARCH(1,1) model as a test for the adequacy of this specification. We review the conditions derived by De Lima (1996, Econometric Reviews, 15, 237-259) for the nuisance-parameter free property to hold, and address the issue of their necessity, using the GARCH(1,1) model. By means of Monte Carlo simulations, we show that, provided that the unconditional mean exists, the BDS test statistic still approximates the standard null distribution even when the majority of the conditions are violated. Further, the test performs reasonably well, as its empirical size is rather close to the nominal one. As a by-product of this study, we also examine the related issue of consistency of the QML estimators of the conditional variance parameters under various parameter configurations and alternative distributional assumptions on the innovation process.},
  keywords={BDS Test; Nuisance-Parameter Free Property; Monte Carlo Analysis; GARCH(1; 1) Model; QML estimator},
  doi={},
}

@article{Funahashi1989,
title = {On the approximate realization of continuous mappings by neural networks},
journal = {Neural Networks},
volume = {2},
number = {3},
pages = {183-192},
year = {1989},
issn = {0893-6080},
doi = {https://doi.org/10.1016/0893-6080(89)90003-8},
url = {https://www.sciencedirect.com/science/article/pii/0893608089900038},
author = {Ken-Ichi Funahashi},
keywords = {Neural network, Back propagation, Output function, Sigmoid function, Hidden layer, Unit, Realization, Continuous mapping},
abstract = {In this paper, we prove that any continuous mapping can be approximately realized by Rumelhart-Hinton-Williams' multilayer neural networks with at least one hidden layer whose output functions are sigmoid functions. The starting point of the proof for the one hidden layer case is an integral formula recently proposed by Irie-Miyake and from this, the general case (for any number of hidden layers) can be proved by induction. The two hidden layers case is proved also by using the Kolmogorov-Arnold-Sprecher theorem and this proof also gives non-trivial realizations.}
}

@inbook{Rumelhart1986,
author = {Rumelhart, D. E. and Hinton, G. E. and Williams, R. J.},
title = {Learning Internal Representations by Error Propagation},
year = {1986},
isbn = {026268053X},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
booktitle = {Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Vol. 1: Foundations},
pages = {318–362},
numpages = {45}
}

@book{Goodfellow2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

@article{McCracken2016,
author = {Michael W. McCracken and Serena Ng},
title = {FRED-MD: A Monthly Database for Macroeconomic Research},
journal = {Journal of Business \& Economic Statistics},
volume = {34},
number = {4},
pages = {574-589},
year  = {2016},
publisher = {Taylor & Francis},
doi = {10.1080/07350015.2015.1086655},

URL = {
        https://doi.org/10.1080/07350015.2015.1086655

},
eprint = {
        https://doi.org/10.1080/07350015.2015.1086655

}

}

@misc{Kingma2017,
      title={Adam: A Method for Stochastic Optimization},
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{Livieris2020,
author = {Livieris, Ioannis and Pintelas, Emmanuel and Pintelas, P.},
year = {2020},
month = {12},
pages = {},
title = {A CNN-LSTM model for gold price time series forecasting},
volume = {32},
journal = {Neural Computing and Applications},
doi = {10.1007/s00521-020-04867-x}
}

